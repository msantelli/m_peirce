# LLM Evaluation Module - Streamlined

Streamlined evaluation system for testing language models on logical reasoning datasets generated by m-peirce-a.

## 🚀 Quick Start

### Basic Usage

```bash
# HuggingFace local models
python evaluate.py --type huggingface --models gpt2

# OpenAI API
python evaluate.py --type openai --models gpt-3.5-turbo --api-key your-key

# Ollama (requires ollama serve)
python evaluate.py --type ollama --models llama3.1

# Simple demo
python simple_example.py
```

### Installation

```bash
pip install -r requirements.txt

# For HuggingFace models (optional)
pip install transformers torch

# For Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
```

## 📊 Supported Model Types

### 1. HuggingFace (`--type huggingface`)
Local and remote models via transformers library
- Automatic device selection (CUDA/CPU)
- Memory-efficient loading

### 2. OpenAI API (`--type openai`)
OpenAI and compatible endpoints
- Custom API endpoints supported
- Built-in retry logic

### 3. Ollama (`--type ollama`)
Local models via Ollama
- Easy model management
- No GPU requirements

## ⚙️ Configuration

### Essential Options
```bash
python evaluate.py \
  --type [ollama|huggingface|openai] \
  --models MODEL1 MODEL2 \
  --prompt-style [standard|enhanced] \
  --timeout 60
```

### Common Configurations
```bash
# Evaluate specific datasets
--datasets balanced_eval_1 balanced_eval_2

# Choose dataset splits  
--splits test validation

# Adjust timeout for slower models
--timeout 120

# Use enhanced prompting
--prompt-style enhanced
```

## 📁 Output Format

Results are saved to timestamped directories:
```
evaluation_results/
└── evaluation_20241210_143052/
    ├── gpt2_balanced_eval_1_test.csv     # Detailed results
    ├── llama3.1_balanced_eval_1_test.csv
    └── evaluation_summary.md             # Summary statistics
```

### CSV Output Columns
- `question_id`: Question identifier
- `model_answer`: Model's response (A, B, UNCLEAR, ERROR)
- `correct_answer`: Expected answer
- `is_correct`: Boolean correctness
- `good_argument_type`: Valid logical rule
- `bad_argument_type`: Corresponding fallacy
- `response_time`: Processing time in seconds
- `raw_response`: Full model output (truncated)
- `parsing_method`: How answer was extracted

## 🛠️ Setup Examples

### HuggingFace Setup
```bash
pip install transformers torch
python evaluate.py --type huggingface --models gpt2
```

### OpenAI Setup
```bash
python evaluate.py --type openai --models gpt-3.5-turbo \
  --api-key sk-your-key --api-base https://api.openai.com/v1
```

### Ollama Setup
```bash
ollama pull llama3.1
ollama serve
python evaluate.py --type ollama --models llama3.1
```

## 🔄 Advanced Usage

### Multiple Models and Datasets
```bash
python evaluate.py --type ollama \
  --models llama3.1 qwen2.5:7b \
  --datasets balanced_eval_1 balanced_eval_2 \
  --splits test validation \
  --prompt-style enhanced
```

### Custom API Endpoints
```bash
python evaluate.py --type openai \
  --models custom-model \
  --api-key your-key \
  --api-base http://localhost:8000/v1
```

### Performance Tuning
```bash
python evaluate.py --type huggingface \
  --models microsoft/DialoGPT-medium \
  --device cuda \
  --max-tokens 20 \
  --temperature 0.0 \
  --timeout 120
```

## 🐛 Common Issues

**"Cannot connect"**: Check if service is running (ollama serve, API endpoint)  
**Timeout errors**: Increase --timeout for slower models  
**Memory issues**: Use smaller models or --device cpu for HuggingFace  
**Import errors**: Install provider-specific dependencies (see Installation)

## 📦 Dependencies

**Core (always needed)**: requests, tqdm  
**HuggingFace**: transformers, torch, accelerate  
**OpenAI**: requests (included in core)  
**Ollama**: requests (included in core)

## 🗂️ Module Structure

```
evaluation/
├── base_evaluator.py     # Core classes and shared logic
├── evaluator.py         # All model provider implementations
├── evaluate.py          # Main CLI script
├── simple_example.py    # Demo script
├── requirements.txt     # Dependencies
├── README.md           # This file
├── archive/            # Legacy files (preserved for reference)
└── evaluation_results/ # Generated results
```

## 🔧 API Usage

For programmatic use:

```python
from evaluator import create_evaluator
from base_evaluator import EvaluationConfig
from pathlib import Path

# Create evaluator
config = EvaluationConfig(model_name="gpt2", timeout=60)
evaluator = create_evaluator("huggingface", config)

# Run evaluation
dataset_file = Path("../outputs/balanced_eval_1/test.jsonl")
stats, results = evaluator.evaluate_dataset(dataset_file)

print(f"Accuracy: {stats.accuracy:.1%}")
```

## 📋 Changelog

**v2.0** - Streamlined architecture (70% code reduction)
- Unified all evaluators into single module
- Consistent API across all providers
- Simplified CLI interface
- Better error handling and progress tracking

Works with existing m-peirce-a .jsonl datasets without changes.