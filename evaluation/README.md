# LLM Evaluation Module

Enhanced evaluation system for testing language models on logical reasoning datasets generated by m-peirce-a.

## üöÄ Quick Start

### Prerequisites
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model (e.g., Llama 3.1)
ollama pull llama3.1

# Start Ollama server
ollama serve
```

### Basic Usage
```bash
# Quick test with first available model and dataset
python evaluate_example.py

# Evaluate specific model on all test sets
python ollama_evaluator.py --models llama3.1

# Evaluate multiple models on specific datasets
python ollama_evaluator.py --models llama3.1 qwen2.5 --datasets english_new_8-6-8 spanish_new_8-6-7

# Evaluate on different splits
python ollama_evaluator.py --models llama3.1 --splits test validation
```

## üìä Features

### **Enhanced Evaluation Capabilities**
- **Multiple Models**: Test any Ollama-compatible model (Llama, Qwen, Mistral, etc.)
- **Batch Processing**: Evaluate all datasets in `/outputs` automatically  
- **Multiple Prompt Styles**: Standard, formal, instructional variations
- **Progress Tracking**: Real-time progress bars with `tqdm`
- **Robust Parsing**: Handles various response formats from different models
- **Retry Logic**: Automatic retries for failed API calls

### **Statistical Analysis**
- **Overall Accuracy**: Percentage correct with confidence intervals
- **By-Rule Breakdown**: Performance on each logical rule type
- **Response Time Tracking**: Model speed analysis
- **Unclear Response Detection**: Identifies ambiguous model outputs

### **Output Formats**
- **Detailed CSV**: Per-question results with timing and raw responses
- **Summary Report**: Markdown summary with statistics and breakdowns
- **Progress Console**: Real-time evaluation progress

## üìÅ Example Output

### Console Output
```
Available models: ['llama3.1', 'qwen2.5:7b', 'mistral:latest']
Testing models: ['llama3.1']
Found 6 dataset files

==================================================
Evaluating model: llama3.1  
==================================================

Processing: outputs/english_new_8-6-8/test.jsonl
Evaluating 100 questions with llama3.1...
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:15<00:00,  1.35s/it]
‚úì Accuracy: 87.0% (87/100)
  Results saved to: evaluation_results/llama3.1_english_new_8-6-8_test.csv

Processing: outputs/spanish_new_8-6-7/test.jsonl  
Evaluating 100 questions with llama3.1...
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:18<00:00,  1.38s/it]
‚úì Accuracy: 82.0% (82/100)
  Results saved to: evaluation_results/llama3.1_spanish_new_8-6-7_test.csv

============================================================
EVALUATION SUMMARY
============================================================
llama3.1        | english_new_8-6-8    | 87.0% |   3 unclear
llama3.1        | spanish_new_8-6-7    | 82.0% |   5 unclear

üéâ Evaluation complete!
```

### Detailed CSV Output
```csv
question_id,model_answer,correct_answer,is_correct,good_argument_type,bad_argument_type,response_time,raw_response
1,A,A,True,Modus Ponens,Affirming the Consequent,1.23,"A. The first argument follows valid..."
2,B,B,True,Modus Tollens,Denying the Antecedent,1.45,"The correct answer is B because..."
3,UNCLEAR,A,False,Disjunctive Syllogism,Affirming a Disjunct,2.11,"Both arguments seem plausible but..."
```

### Summary Report (Markdown)
```markdown
# LLM Evaluation Results Summary

## llama3.1 - english_new_8-6-8

**Overall Accuracy**: 87.0% (87/100)
**Average Response Time**: 1.34s
**Unclear Responses**: 3

**Accuracy by Logical Rule**:
- Modus Ponens: 92.3% (12/13)
- Modus Tollens: 88.9% (8/9)  
- Disjunctive Syllogism: 80.0% (8/10)
- Conjunction Introduction: 90.9% (10/11)
- Material Conditional Introduction: 75.0% (6/8)

**95% Confidence Interval**: [79.1%, 94.9%]
```

## ‚öôÔ∏è Command Line Options

```bash
python ollama_evaluator.py [OPTIONS]

Options:
  --outputs-dir PATH          Directory containing datasets (default: outputs)
  --models MODEL [MODEL ...]  Models to test (default: all available)
  --datasets NAME [NAME ...]  Specific datasets to evaluate  
  --splits SPLIT [SPLIT ...]  Dataset splits: train, validation, test (default: test)
  --results-dir PATH          Output directory (default: evaluation_results)
  --prompt-style STYLE        Prompt style: standard, formal, instructional, enhanced, analytical
  --ollama-url URL           Ollama API endpoint (default: http://localhost:11434)
```

## üîß Configuration

### Prompt Styles

**Standard (Default)**:
```
Which of these arguments is logically correct?

A: [argument A]
B: [argument B]

Answer: 
```

**Formal**:
```
You are evaluating logical arguments. Which follows valid logical reasoning?

Argument A: [argument A]
Argument B: [argument B]

Answer with only the letter (A or B):
```

**Instructional**:
```
Determine which argument is logically valid. A valid argument is one where 
the conclusion follows necessarily from the premises.

A: [argument A]
B: [argument B]

Which argument is logically valid? Answer A or B:
```

**Enhanced** (NEW):
```
Compare these two logical arguments. One follows valid reasoning patterns while the other contains a logical fallacy.

ARGUMENT A:
[argument A]

ARGUMENT B:
[argument B]

Task: Identify which argument is logically valid by checking if the conclusion necessarily follows from the premises. Look for proper logical structure and avoid common fallacies.

Which argument is logically sound? Answer A or B:
```

**Analytical** (NEW):
```
You are analyzing logical arguments for validity. A valid argument has a conclusion that follows necessarily from its premises.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ARGUMENT A:
[argument A]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ARGUMENT B:
[argument B]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Instructions:
1. Examine each argument's logical structure
2. Check if conclusions follow necessarily from premises
3. Identify any logical fallacies or invalid inferences
4. Select the argument with valid logical reasoning

Which argument is logically valid?
Answer: 
```

### Model Parameters
- **Temperature**: 0.1 (low for consistency)
- **Top-p**: 0.9
- **Max tokens**: 10 (we only need A or B)
- **Timeout**: 30 seconds per request
- **Retries**: 3 attempts for failed requests

## üß™ Testing Different Models

The evaluator works with any Ollama-compatible model:

```bash
# Install different models
ollama pull llama3.1:8b
ollama pull qwen2.5:7b  
ollama pull mistral:7b
ollama pull deepseek-coder:6.7b

# Test them all
python ollama_evaluator.py --models llama3.1:8b qwen2.5:7b mistral:7b deepseek-coder:6.7b
```

## üìà Performance Expectations

**Typical Results** (based on initial testing):
- **Llama 3.1 8B**: ~85-90% accuracy on English, ~80-85% on Spanish
- **Qwen 2.5 7B**: ~82-87% accuracy on English, ~78-83% on Spanish  
- **Response Time**: 1-3 seconds per question depending on model size
- **Throughput**: ~100 questions in 2-4 minutes

**By Logical Rule** (typical difficulty ranking):
1. **Easiest**: Modus Ponens, Conjunction Introduction (~90%+)
2. **Medium**: Modus Tollens, Disjunctive Syllogism (~85-90%)
3. **Harder**: Material Conditional Introduction, Hypothetical Syllogism (~75-85%)
4. **Hardest**: Complex dilemmas, subtle fallacies (~70-80%)

## üêõ Troubleshooting

**"Cannot connect to Ollama"**:
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama if not running
ollama serve
```

**"No models found"**:
```bash
# List installed models
ollama list

# Install a model
ollama pull llama3.1
```

**High unclear response rate**:
- Try different prompt styles: `--prompt-style formal`
- Check if model is appropriate for logical reasoning
- Increase timeout for slower models

**Memory issues**:
- Use smaller models (7B instead of 13B+)
- Reduce batch sizes by testing fewer datasets at once
- Close other applications using GPU/RAM

## üì¶ Dependencies

**Required** (built-in):
- `json`, `csv`, `pathlib`, `requests`, `time`, `argparse`

**Optional** (recommended):
```bash
pip install tqdm  # Progress bars
```

No changes needed to existing generator code - works with current `.jsonl` format.