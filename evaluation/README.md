# LLM Evaluation Module

Evaluation system for testing language models on logical reasoning datasets generated by m-peirce-a.

## üöÄ Quick Start

### Basic Usage

```bash
# HuggingFace local models
python unified_evaluator.py --type huggingface --models microsoft/DialoGPT-medium

# OpenAI API
python unified_evaluator.py --type openai --models gpt-3.5-turbo --api-key your-key

# Ollama (requires ollama serve)
python unified_evaluator.py --type ollama --models llama3.1

# Simple example script
python simple_example.py
```

### Installation

```bash
pip install -r requirements.txt

# For HuggingFace models (optional)
pip install transformers torch

# For Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
```

## üìä Supported Model Types

### 1. HuggingFace (`--type huggingface`)
Local and remote models via transformers library
- Automatic device selection (CUDA/CPU)
- Memory-efficient loading

### 2. OpenAI API (`--type openai`)
OpenAI and compatible endpoints
- Custom API endpoints supported
- Built-in retry logic

### 3. Ollama (`--type ollama`)
Local models via Ollama
- Easy model management
- No GPU requirements

## ‚öôÔ∏è Configuration

### Essential Options
```bash
python unified_evaluator.py \
  --type [ollama|huggingface|openai] \
  --models MODEL1 MODEL2 \
  --prompt-style [standard|enhanced] \
  --timeout 60
```

### Common Configurations
```bash
# Evaluate specific datasets
--datasets english_new_8-6-8 spanish_new_8-6-7

# Choose dataset splits  
--splits test validation

# Adjust timeout for slower models
--timeout 120
```

## üìÅ Output Format

Results are saved to timestamped directories:
```
evaluation_results/
‚îî‚îÄ‚îÄ evaluation_20241210_143052/
    ‚îú‚îÄ‚îÄ model1_dataset1_test.csv     # Detailed results
    ‚îú‚îÄ‚îÄ model2_dataset1_test.csv
    ‚îî‚îÄ‚îÄ evaluation_summary.md        # Summary statistics
```

### CSV Output Columns
- `question_id`: Question identifier
- `model_answer`: Model's response (A, B, UNCLEAR, ERROR)
- `correct_answer`: Expected answer
- `is_correct`: Boolean correctness
- `good_argument_type`: Valid logical rule
- `bad_argument_type`: Corresponding fallacy
- `response_time`: Processing time in seconds
- `raw_response`: Full model output (truncated)

## üõ†Ô∏è Setup Examples

### HuggingFace Setup
```bash
pip install transformers torch
python unified_evaluator.py --type huggingface --models gpt2
```

### OpenAI Setup
```bash
python unified_evaluator.py --type openai --models gpt-3.5-turbo \
  --api-key sk-your-key --api-base https://api.openai.com/v1
```

### Ollama Setup
```bash
ollama pull llama3.1
ollama serve
python unified_evaluator.py --type ollama --models llama3.1
```

## üêõ Common Issues

**"Cannot connect"**: Check if service is running (ollama serve, API endpoint)
**Timeout errors**: Increase --timeout for slower models
**Memory issues**: Use smaller models or --device cpu for HuggingFace

## üì¶ Dependencies

**Required**: requests, tqdm (lightweight)
**Optional**: transformers, torch (for HuggingFace models only)

Works with existing m-peirce-a .jsonl datasets without changes.