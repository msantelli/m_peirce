# LLM Evaluation Module

Enhanced evaluation system for testing language models on logical reasoning datasets generated by m-peirce-a.

## 🚀 Quick Start

### Prerequisites
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model (e.g., Llama 3.1)
ollama pull llama3.1

# Start Ollama server
ollama serve
```

### Basic Usage
```bash
# Quick test with first available model and dataset
python evaluate_example.py

# Evaluate specific model on all test sets
python ollama_evaluator.py --models llama3.1

# Evaluate multiple models on specific datasets
python ollama_evaluator.py --models llama3.1 qwen2.5 --datasets english_new_8-6-8 spanish_new_8-6-7

# Evaluate on different splits
python ollama_evaluator.py --models llama3.1 --splits test validation
```

## 📊 Features

### **Enhanced Evaluation Capabilities**
- **Multiple Models**: Test any Ollama-compatible model (Llama, Qwen, Mistral, etc.)
- **Batch Processing**: Evaluate all datasets in `/outputs` automatically  
- **Multiple Prompt Styles**: Standard, formal, instructional variations
- **Progress Tracking**: Real-time progress bars with `tqdm`
- **Robust Parsing**: Handles various response formats from different models
- **Retry Logic**: Automatic retries for failed API calls

### **Statistical Analysis**
- **Overall Accuracy**: Percentage correct with confidence intervals
- **By-Rule Breakdown**: Performance on each logical rule type
- **Response Time Tracking**: Model speed analysis
- **Unclear Response Detection**: Identifies ambiguous model outputs

### **Output Formats**
- **Timestamped Folders**: Each evaluation creates a new folder (e.g., `evaluation_20241210_143052`)
- **Detailed CSV**: Per-question results with timing and raw responses
- **Summary Report**: Markdown summary with statistics and breakdowns
- **Progress Console**: Real-time evaluation progress

## 📁 Output Format

### Console Output Format
Shows real-time progress and results summary:
- Lists available models
- Progress bars for each evaluation
- Accuracy percentages (higher is better)
- File paths where results are saved
- Final summary table

For instance (not real data):
```
Available models: ['model1', 'model2']
Testing models: ['model1']

Processing: outputs/dataset_name/test.jsonl
Evaluating X questions with model1...
✓ Accuracy: XX.X% (correct/total)
  Results saved to: evaluation_results/evaluation_TIMESTAMP/model_dataset_split.csv
```

### Detailed CSV Output Format
Per-question results with these columns:
- `question_id`: Unique identifier for each question
- `model_answer`: Model's response (A, B, UNCLEAR, or ERROR)
- `correct_answer`: Expected correct answer (A or B)
- `is_correct`: Boolean indicating if model was right
- `good_argument_type`: Valid logical rule being tested
- `bad_argument_type`: Corresponding fallacy
- `response_time`: Time in seconds (lower is better)
- `raw_response`: Full model output for debugging

### Summary Report Format (Markdown)
Comprehensive statistics including:
- **Evaluation metadata**: Date, evaluator, total runs
- **Overall accuracy**: Percentage correct (higher is better)
- **Response time**: Average seconds per question (lower is better)
- **Unclear responses**: Count of ambiguous answers (lower is better)
- **By-rule breakdown**: Performance on each logical rule type
- **Confidence intervals**: 95% confidence interval for true accuracy (wider intervals indicate less certainty, narrower intervals with larger sample sizes indicate more reliable estimates)

## ⚙️ Command Line Options

```bash
python ollama_evaluator.py [OPTIONS]

Options:
  --outputs-dir PATH          Directory containing datasets (default: outputs)
  --models MODEL [MODEL ...]  Models to test (default: all available)
  --datasets NAME [NAME ...]  Specific datasets to evaluate  
  --splits SPLIT [SPLIT ...]  Dataset splits: train, validation, test (default: test)
  --results-dir PATH          Output directory (default: evaluation_results)
  --prompt-style STYLE        Prompt style: standard, formal, instructional, enhanced, analytical
  --ollama-url URL           Ollama API endpoint (default: http://localhost:11434)
  --timeout SECONDS          Request timeout in seconds (default: 60)
  --max-retries COUNT        Maximum retry attempts (default: 3)
```

## 🔧 Configuration

### Prompt Styles

**Standard (Default)**:
```
Which of these arguments is logically correct?

A: [argument A]
B: [argument B]

Answer: 
```

**Formal**:
```
You are evaluating logical arguments. Which follows valid logical reasoning?

Argument A: [argument A]
Argument B: [argument B]

Answer with only the letter (A or B):
```

**Instructional**:
```
Determine which argument is logically valid. A valid argument is one where 
the conclusion follows necessarily from the premises.

A: [argument A]
B: [argument B]

Which argument is logically valid? Answer A or B:
```

**Enhanced** (NEW):
```
Compare these two logical arguments. One follows valid reasoning patterns while the other contains a logical fallacy.

ARGUMENT A:
[argument A]

ARGUMENT B:
[argument B]

Task: Identify which argument is logically valid by checking if the conclusion necessarily follows from the premises. Look for proper logical structure and avoid common fallacies.

Which argument is logically sound? Answer A or B:
```

**Analytical** (NEW):
```
You are analyzing logical arguments for validity. A valid argument has a conclusion that follows necessarily from its premises.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ARGUMENT A:
[argument A]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
ARGUMENT B:
[argument B]

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Instructions:
1. Examine each argument's logical structure
2. Check if conclusions follow necessarily from premises
3. Identify any logical fallacies or invalid inferences
4. Select the argument with valid logical reasoning

Which argument is logically valid?
Answer: 
```

### Model Parameters
- **Temperature**: 0.1 (low for consistency)
- **Top-p**: 0.9
- **Max tokens**: 10 (we only need A or B)
- **Timeout**: 60 seconds per request
- **Retries**: 3 attempts for failed requests

## 🧪 Testing Different Models

The evaluator works with any Ollama-compatible model:

```bash
# Install different models
ollama pull llama3.1:8b
ollama pull qwen2.5:7b  
ollama pull mistral:7b
ollama pull deepseek-coder:6.7b

# Test them all
python ollama_evaluator.py --models llama3.1:8b qwen2.5:7b mistral:7b deepseek-coder:6.7b
```

## 📈 Performance Notes

**Response Time**: Varies by model size (smaller models are faster)
**Throughput**: Depends on model complexity and system resources

**Expected Behavior**:
- First request per model is slower (model loading)
- Subsequent requests are faster
- Larger models (13B+) take significantly longer than smaller ones (7B)

**Model Recommendations for Speed**:
- Fast evaluation: `llama3.1:8b`, `qwen2.5:7b`
- Balanced: `mistral:7b`
- Accuracy-focused: Larger models (if you have the resources)

## 🐛 Troubleshooting

**"Cannot connect to Ollama"**:
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Start Ollama if not running
ollama serve
```

**"No models found"**:
```bash
# List installed models
ollama list

# Install a model
ollama pull llama3.1
```

**"Read timed out" / Timeout errors**:
```bash
# Increase timeout (default 60s)
python ollama_evaluator.py --timeout 120 --datasets your_dataset

# For very large models (13B+), use even longer timeout
python ollama_evaluator.py --timeout 300 --datasets your_dataset

# Check system resources
htop  # Monitor CPU/RAM usage
nvidia-smi  # Monitor GPU usage (if applicable)
```

**High unclear response rate**:
- Try different prompt styles: `--prompt-style enhanced`
- Check if model is appropriate for logical reasoning
- Increase timeout for slower models: `--timeout 120`

**Memory issues**:
- Use smaller models (7B instead of 13B+)
- Reduce batch sizes by testing fewer datasets at once
- Close other applications using GPU/RAM
- Free up disk space (models need temporary space)

**Slow evaluation**:
- Model warm-up happens automatically (first request is slowest)
- Use faster models: `ollama pull llama3.1:8b` instead of larger variants
- Check if other processes are using Ollama: `ps aux | grep ollama`

## 📦 Dependencies

**Required** (built-in):
- `json`, `csv`, `pathlib`, `requests`, `time`, `argparse`

**Optional** (recommended):
```bash
pip install tqdm  # Progress bars
```

No changes needed to existing generator code - works with current `.jsonl` format.